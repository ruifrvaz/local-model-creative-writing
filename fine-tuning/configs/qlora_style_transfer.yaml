# QLoRA Configuration for Style Transfer Fine-Tuning
# Optimized for RTX 5090 (32GB VRAM)
# Target: Llama-3.1-8B-Instruct with author's narrative style

base_model: meta-llama/Llama-3.1-8B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# QLoRA Settings (4-bit quantization)
load_in_4bit: true
adapter: qlora
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true

# LoRA Configuration
lora_r: 64                    # Rank: 32=fast/minimal, 64=balanced, 128=maximum
lora_alpha: 128               # Scaling: typically 2x rank
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_fan_in_fan_out: false
# lora_modules_to_save:       # Only needed when adding NEW tokens (e.g., Klingon, emojis)
#   - embed_tokens            # Trains input embeddings (~524M params) - overkill for style transfer
#   - lm_head                 # Trains output predictions (~524M params) - risks overfitting on 36 examples

# Training Data
datasets:
  - path: data/processed/visions_training.jsonl
    type: chat_template  # Using chat format with messages
    field: messages      # Field name in JSONL

# Validation Data (pre-split by 1_prepare_data.py)
# Using explicit validation set to maximize training examples (36 train, 4 val)
val_set_size: 0              # Don't auto-split; using explicit test_datasets
test_datasets:
  - path: data/processed/validation.jsonl
    type: chat_template
    field: messages
dataset_prepared_path: data/processed/

# Training Parameters
sequence_len: 4096           # Max sequence length (our chunks avg ~2600 tokens)
sample_packing: true         # Efficient batching
pad_to_sequence_len: true

learning_rate: 0.0002        # 2e-4 standard QLoRA rate
num_epochs: 3                # 3 epochs for 31 examples
micro_batch_size: 1          # Per-device batch size (long sequences need batch=1)
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8  # Effective batch size = 8
warmup_steps: 10             # ~10% of ~93 steps (31 examples * 3 epochs)

# Optimizer
optimizer: paged_adamw_8bit  # Memory-efficient AdamW
lr_scheduler: cosine
weight_decay: 0.01
max_grad_norm: 1.0

# Efficiency
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
bf16: true                   # Use bfloat16 for RTX 5090
tf32: true                   # TensorFloat-32 acceleration
flash_attention: true        # FlashAttention-2 support

# Evaluation (disabled - using test_datasets for final eval only)
# eval_steps not compatible with val_set_size=0
eval_sample_packing: false

# Logging & Checkpointing
output_dir: checkpoints/qlora-visions-style
save_strategy: epoch         # Save every epoch (small dataset)
save_total_limit: 5          # Keep all checkpoints for comparison
logging_steps: 5             # Log frequently
logging_first_step: true

# Weights & Biases (disabled for now)
# wandb_project: scifi-style-transfer
# wandb_watch:                 # Leave empty to disable
# wandb_log_model:             # Leave empty to disable
# wandb_run_id:                # Leave empty for auto-generation

# Special Tokens (Llama 3.1 format)
# NOTE: Llama 3.1 uses byte-level BPE - no unk_token needed
special_tokens:
  bos_token: "<|begin_of_text|>"
  eos_token: "<|end_of_text|>"
  pad_token: "<|end_of_text|>"

# Chat Template
chat_template: llama3
