# QLoRA Configuration for Style Transfer Fine-Tuning
# Optimized for RTX 5090 (32GB VRAM)
# Target: Llama-3.1-8B-Instruct with author's narrative style

base_model: meta-llama/Llama-3.1-8B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# QLoRA Settings (4-bit quantization)
load_in_4bit: true
adapter: qlora
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true

# LoRA Configuration
lora_r: 64                    # Rank: 32=fast/minimal, 64=balanced, 128=maximum
lora_alpha: 128               # Scaling: typically 2x rank
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_fan_in_fan_out: false
lora_modules_to_save:         # Required when using custom special tokens
  - embed_tokens
  - lm_head

# Training Data
datasets:
  - path: data/processed/training.jsonl
    type: chat_template  # Using chat format with messages
    field: messages      # Field name in JSONL
val_set_size: 0.1            # 10% validation split
dataset_prepared_path: data/processed/

# Training Parameters
sequence_len: 2048           # Max sequence length for training
sample_packing: true         # Efficient batching
pad_to_sequence_len: true

learning_rate: 0.0001        # 1e-4 lower LR for small dataset (avoid overfitting)
num_epochs: 5                # More epochs needed with only 14 examples
micro_batch_size: 2          # Per-device batch size
per_device_train_batch_size: 2  # Smaller batch for small dataset
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4  # Effective batch size = 8
warmup_steps: 20             # Reduced warmup for small dataset

# Optimizer
optimizer: paged_adamw_8bit  # Memory-efficient AdamW
lr_scheduler: cosine
weight_decay: 0.01
max_grad_norm: 1.0

# Efficiency
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
bf16: true                   # Use bfloat16 for RTX 5090
tf32: true                   # TensorFloat-32 acceleration
flash_attention: true        # FlashAttention-2 support

# Evaluation
eval_steps: 10               # Evaluate frequently with small dataset
eval_sample_packing: false

# Logging & Checkpointing
output_dir: checkpoints/qlora-style-pipeline-test
save_strategy: epoch         # Save every epoch (small dataset)
save_total_limit: 5          # Keep all checkpoints for comparison
logging_steps: 5             # Log frequently
logging_first_step: true

# Weights & Biases (disabled for now)
# wandb_project: scifi-style-transfer
# wandb_watch:                 # Leave empty to disable
# wandb_log_model:             # Leave empty to disable
# wandb_run_id:                # Leave empty for auto-generation

# Special Tokens (Llama 3.1 format)
special_tokens:
  bos_token: "<|begin_of_text|>"
  eos_token: "<|end_of_text|>"
  unk_token: "<unk>"
  pad_token: "<|end_of_text|>"

# Chat Template
chat_template: llama3
